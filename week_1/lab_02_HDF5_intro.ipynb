{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bL9HDevd71i"
      },
      "source": [
        "# Storing and Organizing Array Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXjj_LyFd71k"
      },
      "source": [
        "#### A little excursion on how to store, organize and handle large array data sets  ... "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0pWP-_0d71l"
      },
      "source": [
        "#### So far we have used:\n",
        "***NumPy*** files: binary arrays\n",
        "* [save binary: numpy.save()](https://numpy.org/doc/stable/reference/generated/numpy.save.html)\n",
        "* [save multiple arrays: numpy.savez()](https://numpy.org/doc/stable/reference/generated/numpy.savez.html)\n",
        "* [save multiple compressed: numpy.savez.compressed()](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html)\n",
        "* ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-Nc9Fvod71m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "d1 = np.random.random(size = (1000,20))\n",
        "d2 = np.random.random(size = (1000,200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtwYVmmYd71n"
      },
      "outputs": [],
      "source": [
        "#save to binary \n",
        "np.save(\"myArray.npy\", d1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UejQKnpWd71n"
      },
      "outputs": [],
      "source": [
        "#load\n",
        "d3=np.load(\"myArray.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-GhBCmid71n"
      },
      "outputs": [],
      "source": [
        "#check\n",
        "(d1==d3).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO3LsJ7jd71o"
      },
      "source": [
        "## The HDF5 Data Container Format\n",
        "<img src=\"https://github.com/keuperj/DataEngineering22/blob/main/week_1/IMG/HDF_logo.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls-E8Xo0d71o"
      },
      "source": [
        "Hierarchical Data Format (HDF) is a set of file formats (HDF4, HDF5) designed to store and organize large amounts of data with APIs for many programming languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NvyJxdsd71p"
      },
      "source": [
        "#### HDF5 Structure\n",
        "<img src=\"https://github.com/keuperj/DataEngineering22/blob/main/week_1/IMG/hdf5-folder.png?raw=1\" width=800>\n",
        "<font size=5>[Image Source: https://www.sphenisc.com/doku.php/software/development/hdf5-phdf5]</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ftGJA3rd71p"
      },
      "source": [
        "### HDF5 Key Features:\n",
        "* POSIX-like syntax for internal data structures /path/to/resource\n",
        "    * folders\n",
        "    * meta data\n",
        "    * comments (even code)\n",
        "    * arrays \n",
        "* fast $n$-D data access \n",
        "* data compression\n",
        "* APIs for many programming languages "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeEuXQAUd71q"
      },
      "source": [
        "### In Python:\n",
        "* ***h5py***: http://docs.h5py.org/en/stable/index.html\n",
        "* ***HDF5 Docs:*** https://portal.hdfgroup.org/display/support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3--s_rW_d71q"
      },
      "source": [
        "## Creating a Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPmlIfXAd71q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py #this is the HDF5 lib "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "QeJLBX_-d71r"
      },
      "outputs": [],
      "source": [
        "#create some random data\n",
        "matrix1 = np.random.random(size = (1000,1000))\n",
        "matrix2 = np.random.random(size = (10000,100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2gd6Eueyd71r"
      },
      "outputs": [],
      "source": [
        "# write it to the same file - in two different arrays\n",
        "with h5py.File('hdf5_data.h5', 'w') as hdf: #note the write mode 'w'\n",
        "    hdf.create_dataset('dataset1', data=matrix1)\n",
        "    hdf.create_dataset('dataset2', data=matrix2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nToZ4lKUd71r"
      },
      "source": [
        "## Reading "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "FrBXF5pzd71r"
      },
      "outputs": [],
      "source": [
        "#opening, listing and reading files\n",
        "with h5py.File('hdf5_data.h5','r') as hdf:\n",
        "    ls = list(hdf.keys())\n",
        "    print('List of datasets in this file: \\n', ls)\n",
        "    data = hdf.get('dataset2') #here data is still some hdf5 object\n",
        "    dataset1 = np.array(data) #need to convert it into numpy\n",
        "    print('Shape of dataset1: \\n', dataset1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "bvIZPpv2d71s"
      },
      "outputs": [],
      "source": [
        "dataset1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFCt1htFd71s"
      },
      "outputs": [],
      "source": [
        "f = h5py.File('hdf5_data.h5', 'r')\n",
        "ls = list(f.keys())\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "91voidzod71s"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9nIKwtJd71s"
      },
      "source": [
        "## Array Slicing\n",
        "HDF5 support fancy array slicing - so we do not read all data just to get a slice: http://docs.h5py.org/en/latest/high/dataset.html#fancy-indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtDMrd8Yd71t"
      },
      "outputs": [],
      "source": [
        "f = h5py.File('hdf5_data.h5', 'r')\n",
        "f['dataset1'][100:120,:] # this notation mostly follows numpy notation -> try different slices!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb8Y5JATd71t"
      },
      "source": [
        "## Creating Groups\n",
        "We can organize data in groups, just like in file systems where we have files (here datasets) in folders (here groups) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLIHdB8jd71t"
      },
      "outputs": [],
      "source": [
        "matrix1 = np.random.random(size = (1000,1000))\n",
        "matrix2 = np.random.random(size = (1000,1000))\n",
        "matrix3 = np.random.random(size = (1000,1000))\n",
        "matrix4 = np.random.random(size = (1000,1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgve_LQid71t"
      },
      "outputs": [],
      "source": [
        "with h5py.File('hdf5_groups.h5', 'w') as hdf:\n",
        "    G1 = hdf.create_group('Group1')\n",
        "    G1.create_dataset('dataset1', data = matrix1)\n",
        "    G1.create_dataset('dataset4', data = matrix4)\n",
        " \n",
        "    G21 = hdf.create_group('Group2/SubGroup1')\n",
        "    G21.create_dataset('dataset3', data = matrix3)\n",
        "    \n",
        "    G22 = hdf.create_group('Group2/SubGroup2')\n",
        "    G22.create_dataset('dataset2', data = matrix2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1WAfeM7d71t"
      },
      "source": [
        "## Reading Groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "R4vuWoDkd71t"
      },
      "outputs": [],
      "source": [
        "with h5py.File('hdf5_groups.h5','r') as hdf:\n",
        "    base_items = list(hdf.items())\n",
        "    print('Items in the base directory:', base_items)\n",
        "    G2 = hdf.get('Group2')\n",
        "    G2_items = list(G2.items())\n",
        "    print('Items in Group2:', G2_items)\n",
        "    G21 = G2.get('/Group2/SubGroup1')\n",
        "    G21_items = list(G21.items())\n",
        "    print('Items in Group21:', G21_items)\n",
        "    dataset3 = np.array(G21.get('dataset3'))\n",
        "    print(dataset3.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7DV4EQVd71u"
      },
      "source": [
        "### What is happening? Interpret the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzkY3NCCd71u"
      },
      "source": [
        "## Compress Data\n",
        "HDF5 also support native data compression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrfzoHaed71u"
      },
      "outputs": [],
      "source": [
        "matrix1 = np.random.random(size = (1000,1000))\n",
        "matrix2 = np.random.random(size = (1000,1000))\n",
        "matrix3 = np.random.random(size = (1000,1000))\n",
        "matrix4 = np.random.random(size = (1000,1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi7lXZDId71u"
      },
      "outputs": [],
      "source": [
        "with h5py.File('hdf5_groups_compressed.h5', 'w') as hdf:\n",
        "    G1 = hdf.create_group('Group1')\n",
        "    G1.create_dataset('dataset1', data = matrix1, compression=\"gzip\", compression_opts=9)\n",
        "    G1.create_dataset('dataset4', data = matrix4, compression=\"gzip\", compression_opts=9)\n",
        " \n",
        "    G21 = hdf.create_group('Group2/SubGroup1')\n",
        "    G21.create_dataset('dataset3', data = matrix3, compression=\"gzip\", compression_opts=9)\n",
        "    \n",
        "    G22 = hdf.create_group('Group2/SubGroup2')\n",
        "    G22.create_dataset('dataset2', data = matrix2, compression=\"gzip\", compression_opts=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMC0utbdd71u"
      },
      "source": [
        "## Attributes\n",
        "We can add meta information in form of attributes of files, groups and datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBJ-fcCfd71v"
      },
      "outputs": [],
      "source": [
        "matrix1 = np.random.random(size = (1000,1000))\n",
        "matrix2 = np.random.random(size = (10000,100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "scrolled": true,
        "id": "eT1hVb7Kd71v"
      },
      "outputs": [],
      "source": [
        "# Create the HDF5 file\n",
        "hdf = h5py.File('test.h5', 'w')\n",
        "\n",
        "# Create the datasets\n",
        "dataset1 = hdf.create_dataset('dataset1', data=matrix1)\n",
        "dataset2 = hdf.create_dataset('dataset2', data=matrix2)\n",
        "\n",
        "# Set attributes\n",
        "dataset1.attrs['CLASS'] = 'DATA MATRIX'\n",
        "dataset1.attrs['VERSION'] = '1.1'\n",
        "\n",
        "hdf.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "d2Qz96Qed71v"
      },
      "outputs": [],
      "source": [
        "# Read the HDF5 file\n",
        "hdf = h5py.File('test.h5', 'r')\n",
        "ls = list(hdf.keys())\n",
        "print('List of datasets in this file: \\n', ls)\n",
        "data = hdf.get('dataset1')\n",
        "dataset1 = np.array(data)\n",
        "print('Shape of dataset1: \\n', dataset1.shape)\n",
        "#read the attributes\n",
        "k = list(data.attrs.keys())\n",
        "v = list(data.attrs.values())\n",
        "print(k[0])\n",
        "print(v[0])\n",
        "print(data.attrs[k[0]])\n",
        "\n",
        "hdf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxwgEtCAd71v"
      },
      "source": [
        "#### more in the assignment..."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "livereveal": {
      "enable_chalkboard": true,
      "footer": "Janis Keuper - SS21",
      "header": "Data Science: Block 4"
    },
    "colab": {
      "name": "lab_02_HDF5_intro.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}